# üß† Fundamentos de Clasificaci√≥n de Im√°genes: ML cl√°sico vs CNN


## 1. Redes Neuronales Convolucionales (CNN) para Clasificaci√≥n
**¬øQu√© son?**
Imagina que ense√±amos a un ni√±o a reconocer animales:
1. **Primero** ve manchas de color (bordes/texturas) -> **Capas convolucionales iniciales**
2. **Luego** identifica orejas, patas -> **Capas medias**
3. **Finalmente** distingue "perro" vs "gato" -> **Capas finales**

**Partes clave de una CNN**:
- **Capa Convolucional**:
    ```python
    Conv2D(32, (3,3), activation='relu') # 32 filtros de 3x3
    ```

    # Cada filtro detecta patrones distintos (bordes, colores, texturas).
- **Capa Pooling** (ej: MaxPooling):
    ```python
    MaxPooling2D(2,2) # Reduce dimensiones a la mitad

    # Como hacer zoom hacia afuera para ver la imagen mas general.

**Ejemplo completo**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)), # Entrada: imagen 100x100 RGB
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    Flatten(), # Aplana los features para la capa densa
    Dense(128, activation='relu'),
    Dense(2, activation='softmax') # 2 clases: probabilidades
])
```

## 2. Dataset de imagenes
**Estructura ideal**:
```python
/dataset
    /manzanas
        img1.jpg # [Foto de manzana roja]
        img2.jpg # [Foto de manzana verde]
    /platanos
        img1.jpg # [Foto pl√°tano maduro]
```

**Buenas pr√°cticas**:
- **Balance**: 50 imagenes de manzanas y 50 imagenes de pl√°tanos (no 10 vs 90)-
- **Variabilidad**:
    - Diferentes angulos
    - Distintas iluminaciones
    

## 3. Operaciones morfologicas en procesamiento de imagenes
**¬øQu√© son?**

T√©cnicas para modificar la estructura geom√©trica de objetos en imagenes binarias o en escala de grises, basadas en la teor√≠a de conjuntos.
Son fundamentales tanto para el preprocesamiento en ML cl√°sico como para capas iniciales en CNNs. 

### Operaciones b√°sicas (Kernel: `np.ones((3,3))`)

| Operaci√≥n      | Efecto Visual                               | Representaci√≥n | C√≥digo OpenCV                                  | Aplicaci√≥n t√≠pica          |
|----------------|---------------------------------------------|----------------|-----------------------------------------------|----------------------------|
| **Erosi√≥n**    | Adelgaza los objetos, reduce su tama√±o      | ‚¨õ‚Üí‚ñ™Ô∏è        | `cv2.erode(img, kernel, iterations=1)`        | Eliminar ruido peque√±o     |
| **Dilataci√≥n** | Engrosa los objetos, aumenta su tama√±o      | ‚ñ™Ô∏è‚Üí‚¨õ        | `cv2.dilate(img, kernel, iterations=1)`       | Unir partes rotas          |
| **Apertura**   | Elimina salientes finos y ruido exterior    | ‚ö´‚Üíüîò        | `cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)` | Limpieza de fondos         |
| **Cierre**     | Rellena entrantes finos y huecos peque√±os   | üîò‚Üí‚ö´        | `cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)` | Rellenar huecos           |

### Operaciones avanzadas
```python
# Gradiente morfologico (Bordes)
gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)

# Top Hat (Detectar objetos claros sobre fondo oscuro)
tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)

# Black Hat (Detectar objetos oscuros sobre fondo claro)
blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)
```

### **C√≥mo se usan en ML/ CNN?**
- **1.Para ML cl√°sico**:
- Preprocesamiento obligatorio antes de extraer HOG/LBP
- Ejemplo de detecci√≥n de texto:
```python
# Binarizaci√≥n  + apertura para mejorar OCR
_, binary = cv2.treshold(img, 0, 255, cv2.THRESHOLD_BINARY_INV+cv2.TRESH_OTSU)
processed = cv2.morphologyEx(binary, cv2.MORPH_OPEN, np.ones((2,2), np.uint8))
```
- **2.En CNNs**:
- Las primeras capas convolucionales aprenden operaciones similares
- Pueden inicializarse kernels para simular efectos morfol√≥gicos
```python
# Kernel de erosi√≥n aprendible en una CNN
Conv2D(32, (3x3), activation='relu', kernel_initializer='he_normal')
```
- Ejemplo completo: **Limpieza de huellas dactilares**
```python
import cv2
import numpy as np
# 1. Binarizaci√≥n
_, binary = cv2.treshold(img, 128, 255, cv2.THRESHOLD_BINARY_INV)

# 2. Operaciones morfol√≥gicas
kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel) # Rellena huecos
cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel) # Elimina ruido

# 3. Extracci√≥n de caracter√≠sticas
contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
```
**Par√°metros Clave:**

* kernel_size: Mayor tama√±o afecta m√°s la imagen (t√≠pico 3x3 o 5x5)

* iterations: N√∫mero de aplicaciones (usar 1-3 normalmente)

* kernel_shape:

    * cv2.MORPH_RECT (rectangular)

    * cv2.MORPH_ELLIPSE (el√≠ptico)

    * cv2.MORPH_CROSS (cruz)

## 4. Descriptores de im√°genes en OpenCV
**¬øQu√© son?**
Los descriptores son m√©todos matem√°ticos para extraer caracter√≠siticas num√©ricas clave de una imagen, 
permitiendo representar su contenido de forma compacta. Son esenciales para ML cl√°sico, donde el feature engineering es manual.

### Tipos principales de Descriptores
#### 1. **HOG (Histogram of Oriented Gradients)**
**Concepto**:
Analiza la direcci√≥n e intensidad de los gradientes (cambios de color/intensidad) en regiones locales de la imagen.

**C√≥digo**:
```python
import cv2
hog = cv2.HOGDescriptor(
    win_size=(64, 64),  # Tama√±o de la imagen de entrada
    block_size=(16, 16),  # Tama√±o del bloque sobre el que se calcula
    block_stride=(8, 8),  # Desplazamiento entre bloques
    cell_size=(8, 8),     # Tama√±o de la celda para histogramas
    nbins=9               # N√∫mero de bins del histograma
)
features = hog.compute(img)  # Vector de caracter√≠sticas (3780 valores para 64x64)
```
#### 2. **LBP (Local Binary Patterns)**
**Concepto**
Describe texturas comparando cada p√≠xel con sus vecinos, generando un c√≥digo binario.

**C√≥digo**:
```python
from skimage.feature import local_binary_pattern
radius = 1
n_points = 8 * radius
lbp = local_binary_pattern(
    image=gray_img, 
    P=n_points,  # Puntos vecinos a considerar
    R=radius,    # Radio
    method='uniform'  # Patrones uniformes
)
hist, _ = np.histogram(lbp, bins=256, range=(0, 256))
```
**Ejemplo binario**:
```python
P√≠xeles vecinos: [50, 60, 70]  
               [55, 58, 75]  
               [52, 65, 80]  
Umbral (58):    [0, 1, 1]  
                [0, 0, 1]  
                [0, 1, 1]  
LBP Decimal: 01110011 ‚Üí 115
``` 
#### 3. **SIFT y SURF (Detectores de puntos clave)**
**Concepto**:
Identifican puntos invariantes a rotaci√≥n y escala, √∫tiles para objetos con patrones distintivos. 

**Ejemplo**:
```python
sift = cv2.SIFT_create()
keypoints, descriptors = sift.detectAndCompute(img, None)
```
#### 4. **Color histogramas**
**Concepto**:
Describe la distribuci√≥n de colores en la imagen, √∫til para objetos con colores caracter√≠sticos.
```python
hist_b = cv2.calcHist([img], [0], None, [256], [0, 256])  # Canal Azul
hist_g = cv2.calcHist([img], [1], None, [256], [0, 256])  # Canal Verde
hist_r = cv2.calcHist([img], [2], None, [256], [0, 256])  # Canal Rojo
```
### Comparativa de descriptores

| Descriptor | ¬øQu√© es? | Ventajas | Limitaciones | Dimensionalidad T√≠pica | Uso Recomendado |
|------------|----------|----------|--------------|------------------------|-----------------|
| **HOG** (Histograma de Gradientes Orientados) | Captura la distribuci√≥n de direcciones de bordes en la imagen | ‚Ä¢ Robustez a cambios de iluminaci√≥n<br>‚Ä¢ Bueno para detectar formas | ‚Ä¢ Sensible a oclusi√≥n<br>‚Ä¢ No invariante a rotaci√≥n | 3780 (para imagen 64x64) | Detecci√≥n de personas, objetos con forma definida |
| **LBP** (Patrones Binarios Locales) | Analiza la textura comparando cada p√≠xel con sus vecinos | ‚Ä¢ C√≥mputo muy r√°pido<br>‚Ä¢ Excelente para texturas<br>‚Ä¢ Robusto a iluminaci√≥n uniforme | ‚Ä¢ Poco discriminativo para objetos complejos<br>‚Ä¢ Sensible al ruido | 256 (histograma b√°sico) | Reconocimiento facial, clasificaci√≥n de texturas |
| **SIFT** (Transformaci√≥n de Caracter√≠sticas Invariante a Escala) | Detecta puntos clave en la imagen y los describe independientemente de escala y rotaci√≥n | ‚Ä¢ Invariante a rotaci√≥n y escala<br>‚Ä¢ Robusto a cambios de perspectiva<br>‚Ä¢ Altamente distintivo | ‚Ä¢ Computacionalmente costoso<br>‚Ä¢ Anteriormente patentado<br>‚Ä¢ Memoria intensiva | 128 por cada punto clave | Reconocimiento de objetos, stitching de im√°genes |
| **Color Hist** (Histograma de Color) | Cuenta la frecuencia de cada valor de color en la imagen | ‚Ä¢ Simple de implementar<br>‚Ä¢ Intuitivo<br>‚Ä¢ R√°pido de calcular | ‚Ä¢ Ignora informaci√≥n espacial<br>‚Ä¢ Muy sensible a cambios de iluminaci√≥n<br>‚Ä¢ No captura formas | 768 (256 bins √ó 3 canales RGB) | B√∫squeda por contenido, clasificaci√≥n por color | 

## ¬øC√≥mo Usarlos en ML Cl√°sico?

**Pipeline T√≠pico:**

**Preprocesamiento:**
```python
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
equalized = cv2.equalizeHist(gray)
```

**Extracci√≥n:**
```python
hog_features = hog.compute(equalized)
lbp_features = local_binary_pattern(equalized, 8, 1, 'uniform')
```
**Concatenaci√≥n:**
```python
final_features = np.concatenate([hog_features.flatten(), lbp_hist])
```
**Ejemplo con SVM:**
```python
from sklearn.svm import SVC
model = SVC(kernel='rbf')
model.fit(X_train, y_train)  # X_train = [hog_features, lbp_features, ...]
```
## ¬øY en las CNNs?
Las redes neuronales modernas reemplazan los descriptores manuales por:

* Capas convolucionales: Aprenden features autom√°ticamente
* Global Average Pooling: Reduce la dimensionalidad al final

**Ventaja**: No requiere ingenier√≠a manual de caracter√≠sticas, pero necesita m√°s datos.

```python
# Capas iniciales de una CNN act√∫an como "descriptores aprendidos"
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 3)),
    MaxPooling2D(),
    Conv2D(64, (3,3), activation='relu'),
    GlobalAveragePooling2D(),  # Reemplaza a HOG/LBP
    Dense(2, activation='softmax')
])
```
## 5. Machine learning VS CNN

### Machine Learning Cl√°sico (como con una SVM): El Artesano de las Caracter√≠sticas

Piensen en el Machine Learning cl√°sico como si nosotros fu√©ramos artesanos creando las herramientas que el modelo va a usar.

**Preprocesamiento: El Preparado Manual.** Primero, tom√°bamos la imagen y la prepar√°bamos un poco, por ejemplo, convirti√©ndola a blanco y negro para simplificarla. Luego, ven√≠a la parte crucial: **la extracci√≥n manual de caracter√≠sticas**. Era como si tuvi√©ramos un conjunto de reglas o recetas (algoritmos como HOG o LBP) que aplic√°bamos a la imagen para identificar patrones importantes. Por ejemplo, HOG se enfoca en las direcciones de los bordes, y LBP analiza las texturas locales. Nosotros, como ingenieros, decid√≠amos qu√© caracter√≠sticas eran relevantes para el problema.

**Entrenamiento: Aprendizaje con Herramientas Hechas a Mano.** Una vez que ten√≠amos estas "herramientas" (los vectores de caracter√≠sticas HOG o LBP), se las d√°bamos a un modelo de aprendizaje autom√°tico, como una M√°quina de Vectores de Soporte (SVM). La SVM aprend√≠a a clasificar las im√°genes bas√°ndose en estos descriptores que nosotros hab√≠amos creado. Este proceso de entrenamiento sol√≠a ser relativamente r√°pido, incluso en computadoras normales (CPUs), especialmente si no ten√≠amos much√≠simos datos.

**Rendimiento: Limitado por Nuestras Herramientas.** El rendimiento que pod√≠amos obtener depend√≠a mucho de qu√© tan buenas fueran las caracter√≠sticas que hab√≠amos dise√±ado manualmente. Con pocos datos, pod√≠amos obtener resultados decentes, alrededor del 80% de precisi√≥n en algunos casos. Sin embargo, si las im√°genes eran muy complejas o variadas, nuestras "herramientas" manuales pod√≠an quedarse cortas, limitando la precisi√≥n que pod√≠amos alcanzar.

### Redes Neuronales Convolucionales (CNNs): El Aprendiz que Aprende Solo

Ahora, las CNNs son como tener un aprendiz muy inteligente que aprende a identificar las caracter√≠sticas importantes por s√≠ mismo, sin que nosotros tengamos que decirle exactamente qu√© buscar.

**Preprocesamiento: Una Preparaci√≥n Sencilla.** Con las CNNs, la preparaci√≥n inicial de la imagen es mucho m√°s simple. A menudo, solo normalizamos los valores de los p√≠xeles (los hacemos estar en un rango espec√≠fico, como entre 0 y 1). La idea es darle a la red la imagen "casi cruda".

**Entrenamiento: Un Aprendizaje Profundo y Computacionalmente Intensivo.** El "aprendizaje" en las CNNs ocurre a trav√©s de muchas capas de procesamiento (las capas convolucionales). Estas capas aprenden autom√°ticamente a detectar patrones cada vez m√°s complejos a partir de los p√≠xeles de la imagen. Es como si la red construyera sus propias "herramientas" de detecci√≥n de caracter√≠sticas. Este proceso requiere much√≠simos datos y mucha potencia de c√°lculo, por lo que normalmente necesitamos usar tarjetas gr√°ficas especializadas (GPUs) para que el entrenamiento no tarde una eternidad. Puede llevar horas o incluso d√≠as entrenar una CNN potente.

**Rendimiento: Potencialmente Mucho Mayor con Suficientes Datos.** La gran ventaja de las CNNs es que, si les damos suficientes datos, pueden aprender representaciones de las im√°genes mucho m√°s ricas y complejas de lo que nosotros podr√≠amos dise√±ar manualmente. Esto les permite alcanzar niveles de precisi√≥n muy altos, a menudo superando el 95% en tareas de clasificaci√≥n de im√°genes complejas. Sin embargo, esta alta precisi√≥n generalmente viene de la mano de la necesidad de tener una gran cantidad de datos para que la red aprenda correctamente.

## 6. M√©tricas de Evaluaci√≥n

Cuando construimos un modelo de clasificaci√≥n (por ejemplo, para distinguir entre gatos y perros), necesitamos formas de medir qu√© tan bien est√° funcionando. Las **m√©tricas de evaluaci√≥n** nos proporcionan estas medidas. La **matriz de confusi√≥n** es la base para calcular muchas de estas m√©tricas.

### Matriz de Confusi√≥n

La matriz de confusi√≥n es una tabla que resume el rendimiento de un modelo de clasificaci√≥n al comparar las predicciones del modelo con las etiquetas reales de los datos de prueba. Para un problema de clasificaci√≥n binaria (dos clases, como "gato" y "perro"), la matriz de confusi√≥n tiene la siguiente estructura:

|                     | Predicho: Positivo | Predicho: Negativo |
|---------------------|--------------------|--------------------|
| **Real: Positivo** | Verdadero Positivo (TP) | Falso Negativo (FN) |
| **Real: Negativo** | Falso Positivo (FP)    | Verdadero Negativo (TN) |

### 1. Exactitud (Accuracy)

**Descripci√≥n:** La exactitud mide la proporci√≥n de todas las predicciones que fueron correctas. Es la m√©trica m√°s intuitiva, pero puede ser enga√±osa en conjuntos de datos con clases desbalanceadas.

**F√≥rmula:**
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

**C√≥digo:**
```python
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

y_true = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred = [0, 1, 1, 1, 0, 0, 0, 1, 1, 1]

accuracy = accuracy_score(y_true, y_pred)
print(f"Exactitud (Accuracy): {accuracy:.2f}")

cm = confusion_matrix(y_true, y_pred)
print("Matriz de Confusi√≥n:")
print(cm)
```
### 2.Sensibilidad (Recall o Tasa de Verdaderos Positivos - TPR)

**Descripci√≥n:** Mide la proporci√≥n de las instancias positivas reales que fueron correctamente identificadas por el modelo. Es importante cuando queremos minimizar los falsos negativos (casos positivos que el modelo predijo como negativos). Por ejemplo, en la detecci√≥n de enfermedades, queremos un alto recall para no pasar por alto casos positivos.

**F√≥rmula:**
$$\text{Recall} = \frac{TP}{TP + FN}$$
donde:
* TP (True Positives): N√∫mero de instancias positivas que fueron correctamente predichas como positivas.
* FN (False Negatives): N√∫mero de instancias positivas que fueron incorrectamente predichas como negativas.

**C√≥digo:**
```python
from sklearn.metrics import recall_score

# Etiquetas reales (y_true) y predicciones del modelo (y_pred)
# Ejemplo: 1 representa la clase positiva
y_true = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred = [0, 1, 1, 1, 0, 0, 0, 1, 1, 1]

# Calcular el recall para la clase positiva (por defecto, etiqueta 1)
recall_positivo = recall_score(y_true, y_pred)
print(f"Sensibilidad (Recall) para la clase positiva (1): {recall_positivo:.2f}")

# Calcular el recall para una clase espec√≠fica (ej. clase 0)
recall_clase_0 = recall_score(y_true, y_pred, pos_label=0)
print(f"Sensibilidad (Recall) para la clase 0: {recall_clase_0:.2f}")

# Calcular el recall para la clase espec√≠fica (ej. clase 1)
recall_clase_1 = recall_score(y_true, y_pred, pos_label=1)
print(f"Sensibilidad (Recall) para la clase 1: {recall_clase_1:.2f}")
```
### 3.Especificidad (Tasa de Verdaderos Negativos - TNR)

**Descripci√≥n:** Mide la proporci√≥n de las instancias negativas reales que fueron correctamente identificadas por el modelo. Es importante cuando queremos minimizar los falsos positivos (casos negativos que el modelo predijo como positivos). Por ejemplo, en un sistema de alarma de incendios, queremos alta especificidad para evitar falsas alarmas.

**F√≥rmula:**
$$\text{Specificity} = \frac{TN}{TN + FP}$$
donde:
* TN (True Negatives): N√∫mero de instancias negativas que fueron correctamente predichas como negativas.
* FP (False Positives): N√∫mero de instancias negativas que fueron incorrectamente predichas como positivas.

**C√≥digo:**
```python
from sklearn.metrics import confusion_matrix

# Etiquetas reales (y_true) y predicciones del modelo (y_pred)
# Ejemplo: 1 representa la clase positiva, 0 la clase negativa
y_true = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred = [0, 1, 1, 1, 0, 0, 0, 1, 1, 1]

# Calcular la matriz de confusi√≥n
cm = confusion_matrix(y_true, y_pred)
print("Matriz de Confusi√≥n:")
print(cm)
# La matriz de confusi√≥n para clasificaci√≥n binaria es:
# [[TN FP]
#  [FN TP]]

# Extraer los valores de la matriz de confusi√≥n
TN = cm[0, 0]
FP = cm[0, 1]

# Calcular la especificidad
if (TN + FP) == 0:
    specificity = 0  # Evitar divisi√≥n por cero
else:
    specificity = TN / (TN + FP)

print(f"Especificidad: {specificity:.2f}")
```
### 4.Precisi√≥n (Precision)

**Descripci√≥n:** Mide la proporci√≥n de las instancias que el modelo predijo como positivas que realmente fueron positivas. En otras palabras, de todas las veces que el modelo dijo que era "positivo", ¬øcu√°ntas veces acert√≥? La precisi√≥n es importante cuando queremos minimizar los falsos positivos (casos que el modelo clasific√≥ como positivos pero eran negativos). Por ejemplo, en un sistema de recomendaci√≥n, queremos alta precisi√≥n para no recomendar muchos elementos irrelevantes al usuario.

**F√≥rmula:**
$$\text{Precision} = \frac{TP}{TP + FP}$$
donde:
* TP (True Positives): N√∫mero de instancias positivas que fueron correctamente predichas como positivas.
* FP (False Positives): N√∫mero de instancias negativas que fueron incorrectamente predichas como positivas.

**C√≥digo:**
```python
from sklearn.metrics import precision_score

# Etiquetas reales (y_true) y predicciones del modelo (y_pred)
# Ejemplo: 1 representa la clase positiva
y_true = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred = [0, 1, 1, 1, 0, 0, 0, 1, 1, 1]

# Calcular la precisi√≥n para la clase positiva (por defecto, etiqueta 1)
precision_positivo = precision_score(y_true, y_pred)
print(f"Precisi√≥n para la clase positiva (1): {precision_positivo:.2f}")

# Calcular la precisi√≥n para una clase espec√≠fica (ej. clase 0)
precision_clase_0 = precision_score(y_true, y_pred, pos_label=0)
print(f"Precisi√≥n para la clase 0: {precision_clase_0:.2f}")

# Calcular la precisi√≥n para la clase espec√≠fica (ej. clase 1)
precision_clase_1 = precision_score(y_true, y_pred, pos_label=1)
print(f"Precisi√≥n para la clase 1: {precision_clase_1:.2f}")
```
### 5.F1-Score

**Descripci√≥n:** El F1-score es la media arm√≥nica de la precisi√≥n y el recall. Proporciona una medida equilibrada del rendimiento del modelo, especialmente cuando hay un desequilibrio entre precisi√≥n y recall. A diferencia de la media aritm√©tica, la media arm√≥nica penaliza los valores extremos; un F1-score alto solo se logra si tanto la precisi√≥n como el recall son relativamente altos.

**F√≥rmula:**
$$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times TP}{2 \times TP + FP + FN}$$
donde:
* TP (True Positives): N√∫mero de instancias positivas que fueron correctamente predichas como positivas.
* FP (False Positives): N√∫mero de instancias negativas que fueron incorrectamente predichas como positivas.
* FN (False Negatives): N√∫mero de instancias positivas que fueron incorrectamente predichas como negativas.

**C√≥digo:**
```python
from sklearn.metrics import f1_score

# Etiquetas reales (y_true) y predicciones del modelo (y_pred)
# Ejemplo: 1 representa la clase positiva
y_true = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred = [0, 1, 1, 1, 0, 0, 0, 1, 1, 1]

# Calcular el F1-score para la clase positiva (por defecto, etiqueta 1)
f1_positivo = f1_score(y_true, y_pred)
print(f"F1-Score para la clase positiva (1): {f1_positivo:.2f}")

# Calcular el F1-score para una clase espec√≠fica (ej. clase 0)
f1_clase_0 = f1_score(y_true, y_pred, pos_label=0)
print(f"F1-Score para la clase 0: {f1_clase_0:.2f}")

# Calcular el F1-score para la clase espec√≠fica (ej. clase 1)
f1_clase_1 = f1_score(y_true, y_pred, pos_label=1)
print(f"F1-Score para la clase 1: {f1_clase_1:.2f}")
```
### 6.√Årea Bajo la Curva ROC (AUC - ROC)

**Descripci√≥n:** El √Årea Bajo la Curva Caracter√≠stica Operativa del Receptor (AUC-ROC) es una m√©trica de evaluaci√≥n para clasificadores binarios que producen una probabilidad de pertenencia a una clase como salida. La Curva ROC (Receiver Operating Characteristic) se crea graficando la **Tasa de Verdaderos Positivos (TPR o Recall)** en el eje Y contra la **Tasa de Falsos Positivos (FPR)** en el eje X a diferentes umbrales de clasificaci√≥n. El AUC mide el √°rea bajo esta curva.

* Un AUC de **0.5** indica que el clasificador no es mejor que una predicci√≥n aleatoria.
* Un AUC de **1.0** indica un clasificador perfecto.
* Generalmente, cuanto mayor sea el AUC, mejor ser√° el rendimiento del modelo para distinguir entre las dos clases.

**F√≥rmulas:**
$$\text{TPR (Recall)} = \frac{TP}{TP + FN}$$
$$\text{FPR} = \frac{FP}{FP + TN} = 1 - \text{Especificidad}$$

**C√≥digo:**
```python
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np

# Probabilidades predichas por el modelo para la clase positiva
y_prob = np.array([0.1, 0.8, 0.3, 0.9, 0.2, 0.6, 0.4, 0.7, 0.5, 0.95])
# Etiquetas reales (0 para negativo, 1 para positivo)
y_true = np.array([0, 1, 0, 1, 0, 0, 0, 1, 1, 1])

# Calcular el AUC-ROC
auc = roc_auc_score(y_true, y_prob)
print(f"AUC-ROC: {auc:.2f}")

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_true, y_prob)

# Graficar la curva ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal de referencia (clasificador aleatorio)
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
```
### 7.Puntuaci√≥n Logar√≠tmica (Log Loss o Cross-Entropy Loss)

**Descripci√≥n:** La puntuaci√≥n logar√≠tmica, tambi√©n conocida como Log Loss o Cross-Entropy Loss, es una m√©trica de evaluaci√≥n fundamental para clasificadores probabil√≠sticos. A diferencia de m√©tricas como la exactitud que eval√∫an las predicciones discretas finales, el Log Loss tiene en cuenta la **incertidumbre** de las predicciones del modelo. Penaliza las predicciones incorrectas con mayor severidad cuanto m√°s confiado est√© el modelo en su predicci√≥n err√≥nea. Un valor de Log Loss m√°s bajo indica un mejor ajuste del modelo a los datos.

**F√≥rmula (para clasificaci√≥n binaria):**
$$\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]$$
donde:
* $N$ es el n√∫mero total de muestras.
* $y_i$ es la etiqueta real de la $i$-√©sima muestra (0 o 1).
* $p_i$ es la probabilidad predicha por el modelo de que la $i$-√©sima muestra pertenezca a la clase 1.

**Para clasificaci√≥n multiclase:**
$$\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{M} y_{ij} \log(p_{ij})$$
donde:
* $M$ es el n√∫mero de clases.
* $y_{ij}$ es 1 si la $i$-√©sima muestra pertenece a la clase $j$, y 0 en caso contrario (codificaci√≥n one-hot).
* $p_{ij}$ es la probabilidad predicha por el modelo de que la $i$-√©sima muestra pertenezca a la clase $j$.

**C√≥digo:**
```python
from sklearn.metrics import log_loss
import numpy as np

# Ejemplo de clasificaci√≥n binaria
y_true_binary = np.array([0, 1, 0, 1, 0, 0, 1, 1])
y_prob_binary = np.array([0.1, 0.8, 0.2, 0.9, 0.7, 0.3, 0.6, 0.95])

logloss_binary = log_loss(y_true_binary, y_prob_binary)
print(f"Log Loss (Binario): {logloss_binary:.2f}")

# Ejemplo de clasificaci√≥n multiclase
y_true_multiclass = np.array([0, 1, 2, 0, 1, 2])
y_prob_multiclass = np.array([
    [0.7, 0.2, 0.1],
    [0.1, 0.8, 0.1],
    [0.2, 0.3, 0.5],
    [0.6, 0.3, 0.1],
    [0.05, 0.9, 0.05],
    [0.3, 0.4, 0.3]
])

logloss_multiclass = log_loss(y_true_multiclass, y_prob_multiclass)
print(f"Log Loss (Multiclase): {logloss_multiclass:.2f}")
```
### 8.Kappa de Cohen

**Descripci√≥n:** El Kappa de Cohen ($\kappa$) es una m√©trica estad√≠stica que mide la concordancia entre las predicciones de un clasificador y las etiquetas reales, corrigiendo la concordancia esperada por azar. En otras palabras, eval√∫a cu√°nto mejor es el rendimiento del modelo en comparaci√≥n con lo que se esperar√≠a si las predicciones se hicieran completamente al azar. El Kappa de Cohen es √∫til cuando se trabaja con datos desbalanceados o cuando existe un componente subjetivo en el etiquetado.

**Rango de Valores:**
* $\kappa = 1$: Concordancia perfecta entre el clasificador y las etiquetas.
* $\kappa = 0$: Concordancia igual a la que se esperar√≠a por azar.
* $\kappa < 0$: Concordancia menor de la esperada por azar (lo cual es raro y generalmente indica un problema con el modelo o los datos).

**F√≥rmula:**
$$\kappa = \frac{p_o - p_e}{1 - p_e}$$
donde:
* $p_o$ es la proporci√≥n de concordancia observada (que es simplemente la exactitud del clasificador).
* $p_e$ es la proporci√≥n de concordancia esperada por azar. Se calcula considerando las probabilidades marginales de cada clase en las etiquetas reales y las predicciones.

Para un problema de clasificaci√≥n binaria, $p_e$ se puede calcular como:
$$p_e = P(\text{clase 1 real}) \times P(\text{clase 1 predicha}) + P(\text{clase 0 real}) \times P(\text{clase 0 predicha})$$

**C√≥digo:**
```python
from sklearn.metrics import cohen_kappa_score
import numpy as np

# Etiquetas reales y predicciones del modelo
y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
y_pred = np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1])

# Calcular el Kappa de Cohen
kappa = cohen_kappa_score(y_true, y_pred)
print(f"Kappa de Cohen: {kappa:.2f}")

# Ejemplo con mayor desacuerdo
y_pred_malo = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])
kappa_malo = cohen_kappa_score(y_true, y_pred_malo)
print(f"Kappa de Cohen (mayor desacuerdo): {kappa_malo:.2f}")

# Ejemplo con concordancia perfecta
y_pred_perfecto = y_true
kappa_perfecto = cohen_kappa_score(y_true, y_pred_perfecto)
print(f"Kappa de Cohen (concordancia perfecta): {kappa_perfecto:.2f}")

# Ejemplo con concordancia al azar (aproximada)
y_pred_azar = np.random.randint(0, 2, size=len(y_true))
kappa_azar = cohen_kappa_score(y_true, y_pred_azar)
print(f"Kappa de Cohen (azar aproximado): {kappa_azar:.2f}")
```
## 7. Normalizaci√≥n vs Estandarizaci√≥n

En el preprocesamiento de datos para Machine Learning, tanto la normalizaci√≥n como la estandarizaci√≥n son t√©cnicas comunes para escalar los valores de las caracter√≠sticas. El objetivo es asegurar que diferentes caracter√≠sticas con diferentes rangos no afecten desproporcionadamente el rendimiento del modelo.

**Normalizaci√≥n:**

* **Objetivo:** Escalar los valores de las caracter√≠sticas a un rango espec√≠fico, generalmente entre 0 y 1.
* **M√©todo Com√∫n:** Dividir cada valor por el valor m√°ximo de la caracter√≠stica (o el rango si se conoce). Para im√°genes con valores de p√≠xeles entre 0 y 255, la normalizaci√≥n a \[0, 1] se logra dividiendo cada valor de p√≠xel por 255.
* **Resultado:** Los datos normalizados estar√°n confinados dentro de un intervalo fijo.
* **Sensibilidad a Outliers:** Es sensible a los valores at√≠picos (outliers). Si hay un valor m√°ximo muy grande, la mayor√≠a de los otros valores se comprimir√°n en un rango peque√±o.

**Estandarizaci√≥n (o Z-score Normalization):**

* **Objetivo:** Escalar los valores de las caracter√≠sticas para que tengan una media de aproximadamente 0 y una desviaci√≥n est√°ndar de aproximadamente 1.
* **M√©todo Com√∫n:** Para cada valor $x$ de una caracter√≠stica, se calcula $(x - \mu) / \sigma$, donde $\mu$ es la media de la caracter√≠stica y $\sigma$ es su desviaci√≥n est√°ndar.
* **Resultado:** Los datos estandarizados no tienen un rango fijo. Pueden tener valores positivos y negativos, y no est√°n necesariamente confinados a un intervalo espec√≠fico.
* **Menos Sensibilidad a Outliers:** Es menos sensible a los outliers en comparaci√≥n con la normalizaci√≥n basada en el rango, ya que la media y la desviaci√≥n est√°ndar son menos dr√°sticamente afectadas por valores extremos. Sin embargo, los outliers a√∫n pueden influir en la media y la desviaci√≥n est√°ndar.

**¬øCu√°ndo usar cu√°l?**

* **Normalizaci√≥n (img/255): Siempre para CNNs.**
    * Las Redes Neuronales Convolucionales (CNNs) a menudo funcionan mejor con datos de entrada normalizados al rango \[0, 1] o \[-1, 1]. La divisi√≥n por 255 es una forma com√∫n y efectiva de normalizar los valores de p√≠xeles de im√°genes al rango \[0, 1]. Esto ayuda a que el proceso de aprendizaje sea m√°s estable y r√°pido, ya que los valores de entrada se mantienen dentro de un rango manejable para las funciones de activaci√≥n y los c√°lculos de gradiente.

* **Estandarizaci√≥n: Para SVM/Random Forest con features HOG.**
    * Algoritmos como las M√°quinas de Vectores de Soporte (SVM) y los Bosques Aleatorios (Random Forest) pueden beneficiarse de la estandarizaci√≥n cuando se utilizan caracter√≠sticas como Histogram of Oriented Gradients (HOG). HOG produce vectores de caracter√≠sticas con valores en un rango que puede variar significativamente entre diferentes im√°genes y celdas. La estandarizaci√≥n asegura que cada componente del vector de caracter√≠sticas tenga una escala similar, lo que puede mejorar el rendimiento de estos modelos.
    * SVM es sensible a la escala de las caracter√≠sticas, ya que busca maximizar el margen basado en las distancias. La estandarizaci√≥n evita que las caracter√≠sticas con rangos m√°s grandes dominen el c√°lculo de la distancia.
    * Si bien los Bosques Aleatorios son menos sensibles a la escala de las caracter√≠sticas que SVM, la estandarizaci√≥n de las caracter√≠sticas HOG puede, en algunos casos, conducir a una convergencia m√°s r√°pida o un rendimiento ligeramente mejor.

**En resumen:** La elecci√≥n entre normalizaci√≥n y estandarizaci√≥n depende del algoritmo de Machine Learning que se est√© utilizando y de la naturaleza de los datos. La normalizaci√≥n al rango \[0, 1] es una pr√°ctica est√°ndar para la entrada de im√°genes en CNNs, mientras que la estandarizaci√≥n puede ser preferible para otros algoritmos como SVM y Random Forest, especialmente cuando se utilizan descriptores de caracter√≠sticas como HOG.


# Desglose de t√©rminos
## 1.`Conv2D`
Es una capa convolucional 2D. Se usa para procesar imagenes (que son datos bidimensionales con 1 o m√°s canales, como RGB). Esta capa:
* Extrae caracter√≠sticas espaciales de la imagen (como bordes, texturas, formas).
* Usa unos "filtros" o "kernels" que van desplazandose (convolucionando) sobre la imagen original.

**Ejemplo**: `Conv2D(32, (3,3), activation='relu')`
- `32` (n√∫mero de filtros)
* Significa que esta capa aplicar√° 32 filtros distintos
* Cada filtro va a detectar un tipo de patr√≥n distinto (un filtro puede detectar bordes verticales, otro horizontales, texturas etc)
* **Resultado**: Se generan 32 mapas de activaci√≥n, uno por cada filtro. Si tu imagen era `(100, 100, 3)`, la salida de esta capa ser√° algo como
`(98, 98, 32)` que ahora es una representaci√≥n abstracta de caracter√≠sticas aprendidas. (esto depende del tama√±o del filtro y si hay padding o no).
- `(3,3)` (tama√±o del filtro o kernel)
* Cada filtro es una matriz de tama√±o 3x3 que "lee" una peque√±a parte de la imagen.
* El filtro se va desplazando (como una ventana) por toda la imagen.
* Se calcula un valor de salida por cada posici√≥n, generando un mapa. 
* Esto se llama **operaci√≥n de convoluci√≥n**.
`activation='relu'`
* Despu√©s de aplicar la convoluci√≥n, se pasa el resultado por una funci√≥n de activaci√≥n.
* `ReLU` (Rectified Linear Unit) es la activaci√≥n m√°s com√∫n en CNNs.
* Su formular es simple: `f(x) = max(0, x)`
    * Todos los valores negativos se convierten en 0
    * Se conservan los positivos
Esto introduce **no linealidad**, lo que permite a la red aprender patrones complejos.

## 2.`MaxPooling2D(2,2)`
- Max Pooling es una operaci√≥n que:
    * **Reduce el tama√±o espacial** (alto y ancho) de los mapas de activaci√≥n.
    * Se aplica **independientemente por cada canal**.
    * Su funci√≥n principal es:
        * **Reducir la cantidad de datos**(y por ende, el c√≥mputo).
        * **Conservar las caracter√≠sticas m√°s importantes**(como bordes, texturas clave).
        * Hacer que la red sea m√°s **robusta a peque√±as variaciones** en la imagen (como movimientos o ruidos).
    * Reduce el overfitting: Al reducir la cantidad de par√°metros que la red necesira procesar.
    * Hace que la red generalice mejor: Porque conserva solo los aspectos m√°s prominentes (los valores m√°ximos).
    * Reduce el tiempo de entrenamiento y mejora la invariancia espacial, es decir si un objeto se mueve un poco, igual puede ser detectado.

**Explicaci√≥n no t√©cnica del ejemplo completo en punto 1**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)), # Entrada: imagen 100x100 RGB
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    Flatten(), # Aplana los features para la capa densa
    Dense(128, activation='relu'),
    Dense(2, activation='softmax') # 2 clases: probabilidades
])
```
Imagina que est√°s entrenando a un robot a reconocer im√°genes de manzanas y pl√°tanos. Este es el recorrido que hace paso a paso:

üîπ Paso 1: Mira la imagen por partes peque√±as
üß© `Conv2D`
El robot examina peque√±os bloques de 3x3 p√≠xeles, buscando formas b√°sicas como bordes y l√≠neas.

üîπ Paso 2: Se queda solo con lo importante
üîç `MaxPooling2D`
En cada zona, se queda con lo m√°s representativo (el valor m√°s fuerte), resumiendo la imagen sin perder lo esencial.

üîπ Paso 3: Vuelve a analizar con m√°s experiencia
üéØ `Conv2D` con m√°s filtros
Ya con una idea general, observa con mayor profundidad y m√°s filtros. Ahora reconoce formas completas como la curvatura de un pl√°tano o el tallo de una manzana.

üîπ Paso 4: Junta todo lo que aprendi√≥
üì¶ `Flatten`
Convierte toda esa informaci√≥n visual en una lista lineal para tomar decisiones.

üîπ Paso 5: Toma una decisi√≥n
üß† `Dense`
Con todo lo aprendido, el robot pasa por una etapa de "reflexi√≥n" (una capa densa) y finalmente decide:
‚Üí ‚ÄúEstoy 90% seguro de que es una manzana y 10% de que es un pl√°tano‚Äù.

