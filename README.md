# üß† Fundamentos de Clasificaci√≥n de Im√°genes: ML cl√°sico vs CNN


## 1. Redes Neuronales Convolucionales (CNN) para Clasificaci√≥n
**¬øQu√© son?**
Imagina que ense√±amos a un ni√±o a reconocer animales:
1. **Primero** ve manchas de color (bordes/texturas) -> **Capas convolucionales iniciales**
2. **Luego** identifica orejas, patas -> **Capas medias**
3. **Finalmente** distingue "perro" vs "gato" -> **Capas finales**

**Partes clave de una CNN**:
- **Capa Convolucional**:
    ```python
    Conv2D(32, (3,3), activation='relu') # 32 filtros de 3x3
    ```

    # Cada filtro detecta patrones distintos (bordes, colores, texturas).
- **Capa Pooling** (ej: MaxPooling):
    ```python
    MaxPooling2D(2,2) # Reduce dimensiones a la mitad

    # Como hacer zoom hacia afuera para ver la imagen mas general.

**Ejemplo completo**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)), # Entrada: imagen 100x100 RGB
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    Flatten(), # Aplana los features para la capa densa
    Dense(128, activation='relu'),
    Dense(2, activation='softmax') # 2 clases: probabilidades
])
```

## 2. Dataset de imagenes
**Estructura ideal**:
```python
/dataset
    /manzanas
        img1.jpg # [Foto de manzana roja]
        img2.jpg # [Foto de manzana verde]
    /platanos
        img1.jpg # [Foto pl√°tano maduro]
```

**Buenas pr√°cticas**:
- **Balance**: 50 imagenes de manzanas y 50 imagenes de pl√°tanos (no 10 vs 90)-
- **Variabilidad**:
    - Diferentes angulos
    - Distintas iluminaciones
    ```

## 3. Operaciones morfologicas en procesamiento de imagenes
**¬øQu√© son?**

T√©cnicas para modificar la estructura geom√©trica de objetos en imagenes binarias o en escala de grises, basadas en la teor√≠a de conjuntos.
Son fundamentales tanto para el preprocesamiento en ML cl√°sico como para capas iniciales en CNNs. 

### Operaciones b√°sicas (Kernel: `np.ones((3,3))`)

| Operaci√≥n      | Efecto Visual                               | Representaci√≥n | C√≥digo OpenCV                                  | Aplicaci√≥n t√≠pica          |
|----------------|---------------------------------------------|----------------|-----------------------------------------------|----------------------------|
| **Erosi√≥n**    | Adelgaza los objetos, reduce su tama√±o      | ‚¨õ‚Üí‚ñ™Ô∏è        | `cv2.erode(img, kernel, iterations=1)`        | Eliminar ruido peque√±o     |
| **Dilataci√≥n** | Engrosa los objetos, aumenta su tama√±o      | ‚ñ™Ô∏è‚Üí‚¨õ        | `cv2.dilate(img, kernel, iterations=1)`       | Unir partes rotas          |
| **Apertura**   | Elimina salientes finos y ruido exterior    | ‚ö´‚Üíüîò        | `cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)` | Limpieza de fondos         |
| **Cierre**     | Rellena entrantes finos y huecos peque√±os   | üîò‚Üí‚ö´        | `cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)` | Rellenar huecos           |

### Operaciones avanzadas
```python
# Gradiente morfologico (Bordes)
gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)

# Top Hat (Detectar objetos claros sobre fondo oscuro)
tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)

# Black Hat (Detectar objetos oscuros sobre fondo claro)
blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)
```

### **C√≥mo se usan en ML/ CNN?**
- **1.Para ML cl√°sico**:
- Preprocesamiento obligatorio antes de extraer HOG/LBP
- Ejemplo de detecci√≥n de texto:
```python
# Binarizaci√≥n  + apertura para mejorar OCR
_, binary = cv2.treshold(img, 0, 255, cv2.THRESHOLD_BINARY_INV+cv2.TRESH_OTSU)
processed = cv2.morphologyEx(binary, cv2.MORPH_OPEN, np.ones((2,2), np.uint8))
```
- **2.En CNNs**:
- Las primeras capas convolucionales aprenden operaciones similares
- Pueden inicializarse kernels para simular efectos morfol√≥gicos
```python
# Kernel de erosi√≥n aprendible en una CNN
Conv2D(32, (3x3), activation='relu', kernel_initializer='he_normal')
```

# Desglose de t√©rminos
## 1.`Conv2D`
Es una capa convolucional 2D. Se usa para procesar imagenes (que son datos bidimensionales con 1 o m√°s canales, como RGB). Esta capa:
* Extrae caracter√≠sticas espaciales de la imagen (como bordes, texturas, formas).
* Usa unos "filtros" o "kernels" que van desplazandose (convolucionando) sobre la imagen original.

**Ejemplo**: `Conv2D(32, (3,3), activation='relu')`
- `32` (n√∫mero de filtros)
* Significa que esta capa aplicar√° 32 filtros distintos
* Cada filtro va a detectar un tipo de patr√≥n distinto (un filtro puede detectar bordes verticales, otro horizontales, texturas etc)
* **Resultado**: Se generan 32 mapas de activaci√≥n, uno por cada filtro. Si tu imagen era `(100, 100, 3)`, la salida de esta capa ser√° algo como
`(98, 98, 32)` que ahora es una representaci√≥n abstracta de caracter√≠sticas aprendidas. (esto depende del tama√±o del filtro y si hay padding o no).
- `(3,3)` (tama√±o del filtro o kernel)
* Cada filtro es una matriz de tama√±o 3x3 que "lee" una peque√±a parte de la imagen.
* El filtro se va desplazando (como una ventana) por toda la imagen.
* Se calcula un valor de salida por cada posici√≥n, generando un mapa. 
* Esto se llama **operaci√≥n de convoluci√≥n**.
`activation='relu'`
* Despu√©s de aplicar la convoluci√≥n, se pasa el resultado por una funci√≥n de activaci√≥n.
* `ReLU` (Rectified Linear Unit) es la activaci√≥n m√°s com√∫n en CNNs.
* Su formular es simple: `f(x) = max(0, x)`
    * Todos los valores negativos se convierten en 0
    * Se conservan los positivos
Esto introduce **no linealidad**, lo que permite a la red aprender patrones complejos.

## 2.`MaxPooling2D(2,2)`
- Max Pooling es una operaci√≥n que:
    * **Reduce el tama√±o espacial** (alto y ancho) de los mapas de activaci√≥n.
    * Se aplica **independientemente por cada canal**.
    * Su funci√≥n principal es:
        * **Reducir la cantidad de datos**(y por ende, el c√≥mputo).
        * **Conservar las caracter√≠sticas m√°s importantes**(como bordes, texturas clave).
        * Hacer que la red sea m√°s **robusta a peque√±as variaciones** en la imagen (como movimientos o ruidos).
    * Reduce el overfitting: Al reducir la cantidad de par√°metros que la red necesira procesar.
    * Hace que la red generalice mejor: Porque conserva solo los aspectos m√°s prominentes (los valores m√°ximos).
    * Reduce el tiempo de entrenamiento y mejora la invariancia espacial, es decir si un objeto se mueve un poco, igual puede ser detectado.

**Explicaci√≥n no t√©cnica del ejemplo completo en punto 1**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)), # Entrada: imagen 100x100 RGB
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    Flatten(), # Aplana los features para la capa densa
    Dense(128, activation='relu'),
    Dense(2, activation='softmax') # 2 clases: probabilidades
])
```
Imagina que est√°s entrenando a un robot a reconocer im√°genes de manzanas y pl√°tanos. Este es el recorrido que hace paso a paso:

üîπ Paso 1: Mira la imagen por partes peque√±as
üß© `Conv2D`
El robot examina peque√±os bloques de 3x3 p√≠xeles, buscando formas b√°sicas como bordes y l√≠neas.

üîπ Paso 2: Se queda solo con lo importante
üîç `MaxPooling2D`
En cada zona, se queda con lo m√°s representativo (el valor m√°s fuerte), resumiendo la imagen sin perder lo esencial.

üîπ Paso 3: Vuelve a analizar con m√°s experiencia
üéØ `Conv2D` con m√°s filtros
Ya con una idea general, observa con mayor profundidad y m√°s filtros. Ahora reconoce formas completas como la curvatura de un pl√°tano o el tallo de una manzana.

üîπ Paso 4: Junta todo lo que aprendi√≥
üì¶ `Flatten`
Convierte toda esa informaci√≥n visual en una lista lineal para tomar decisiones.

üîπ Paso 5: Toma una decisi√≥n
üß† `Dense`
Con todo lo aprendido, el robot pasa por una etapa de "reflexi√≥n" (una capa densa) y finalmente decide:
‚Üí ‚ÄúEstoy 90% seguro de que es una manzana y 10% de que es un pl√°tano‚Äù.